\documentclass[hidelinks]{report}
\usepackage{graphicx} % Required for inserting images
\usepackage{amsmath}
\usepackage{siunitx}
\usepackage{placeins}
\usepackage{float}
\usepackage{hyperref}
\usepackage{cite}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{xcolor,graphicx}
\setcounter{secnumdepth}{0}
\usepackage{titlesec}
\usepackage[left=1.5in,top=1in,bottom=1in,right=1in]{geometry}
\usepackage{rotating}
\usepackage{subcaption}
\usepackage{lipsum}
\usepackage{fancyhdr}
\usepackage{mathptmx}
\usepackage{listings}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{adjustbox}
\usepackage{enumitem}
\usepackage{setspace}
\usepackage{multirow}
\usepackage{tikz}
\setcounter{tocdepth}{4}
\usepackage{amsfonts}
\usepackage{pdflscape}
\usepackage{booktabs} 
\usepackage{array} 

\newcommand{\mynote}[2]{\fbox{\bfseries\sffamily\scriptsize{#1}} {\small\textsf{\emph{#2}}}}

\newcommand{\hieplnc}[1]{\textcolor{red}{\mynote{hieplnc}{#1}}}

\newcommand{\sontg}[1]{\textcolor{blue}{\mynote{sontg}{#1}}}

\definecolor{blue}{RGB}{31,56,100}

\usepackage{lipsum}% http://ctan.org/pkg/lipsum
\makeatletter
\def\@makechapterhead#1{%
  {%
    \parindent \z@ \normalfont % No specific alignment for full justification
    
    \ifnum \c@secnumdepth >\m@ne
        \LARGE\bfseries \thechapter.\ % Changed to \Large (smaller than \LARGE)
    \fi
    \interlinepenalty\@M
    \LARGE\bfseries % Reduced from \LARGE to \Large
    #1\par\nobreak% <------------------ Chapter title
    \vskip 40\p@% <------------------ Space between chapter title and first paragraph
  }%
}

\def\@makeschapterhead#1{%
  {%
    \parindent \z@ \normalfont % No specific alignment for full justification
    \interlinepenalty\@M
    \LARGE\bfseries % Reduced from \LARGE to \Large
    #1\par\nobreak% <------------------ Chapter title
    \vskip 40\p@% <------------------ Space between chapter title and first paragraph
  }%
}
\makeatother

% Redefine the \thesection and \thesubsection representations
\renewcommand{\thesection}{\arabic{chapter}.\arabic{section}}
\renewcommand{\thesubsection}{\thesection.\arabic{subsection}}
\renewcommand{\thesubsubsection}{\thesubsection.\arabic{subsubsection}}

% Define a new counter for subsections
% \newcounter{subsecindex}[section]
% \renewcommand{\thesubsecindex}{\thesubsection%
%   \ifnum\value{subsecindex}>0
%     .\arabic{subsecindex}%
%   \fi
% }

% Redefine the \section command to include the index
% \let\oldsection\section
% \renewcommand{\section}[1]{%
%   \setcounter{subsecindex}{0} % Reset subsection counter for each section
%   \refstepcounter{section}%
%   \oldsection{\thesection\hspace{0.5em}#1}%
% }

% Redefine the \subsection command to include the index
% \let\oldsubsection\subsection
% \renewcommand{\subsection}[1]{%
%   \refstepcounter{subsection}%
%   \oldsubsection{\thesubsecindex\hspace{0.5em}#1}%
% }

% Redefine the \subsubsection command to include the index
% \let\oldsubsubsection\subsubsection
% \renewcommand{\subsubsection}[1]{%
%   \refstepcounter{subsubsection}%
%   \oldsubsubsection{\thesubsecindex\hspace{0.5em}#1}%
% }

\titleformat{\section}
  {\normalfont\LARGE\bfseries} % Adjust \Large to any size you prefer
  {\thesection}{3em}{}
\titleformat{\subsection}
  {\normalfont\Large\bfseries} % Adjust \Large to any size you prefer
  {\thesubsection}{3em}{}
\titleformat{\subsubsection}
  {\normalfont\Large\bfseries} % Adjust \Large to any size you prefer
  {\thesubsubsection}{3em}{}

\begin{document}
\pagenumbering{gobble}

\pdfbookmark[0]{Main Title}{maintitle}
\begin{titlepage}
    \begin{tikzpicture}[remember picture,overlay,inner sep=0,outer sep=0]
        \draw[black!70!black,line width=1.5pt]
            ([xshift=-0.65in,yshift=-1cm]current page.north east) coordinate (A) -- % Adjusted x-shift and y-shift
            ([xshift=0.65in,yshift=-1cm]current page.north west) coordinate (B) -- % Adjusted x-shift and y-shift
            ([xshift=0.65in,yshift=1cm]current page.south west) coordinate (C) -- % Adjusted x-shift and y-shift
            ([xshift=-0.65in,yshift=1cm]current page.south east) -- % Adjusted x-shift
            cycle;
    \end{tikzpicture}

    \begin{center}
        \huge \uppercase{University of Science and Technology of Hanoi} \\ [1.5 cm]
    
        \includegraphics[width=0.9\linewidth]{image/usth.png} \\[1cm]

        {\huge \bfseries \uppercase{MASTER LABWORK}} \\[1cm]

        {\large \bfseries Nguyen Viet Tung} \\ [0.5cm]
        {\large \bfseries 2440049} \\ [0.7cm]
        {\huge \bfseries \uppercase{Deep Learning}} \\[1cm]
        
        % Title
        \rule{\linewidth}{0.3mm} \\[0.4cm]
        { \Huge \bfseries\color{blue} Labwork 2: Linear Regression \\[0.4cm] }
        \rule{\linewidth}{0.3mm} \\[0.7cm]
        
        \large Academic Year: 2024-2026
    \end{center}

\end{titlepage}

\newpage
\pagenumbering{roman}
\tableofcontents

\newpage
\listoffigures

\thispagestyle{empty}
\newpage
\pagenumbering{arabic}

% Keep the preamble and title page sections as they are...

\begin{abstract}
        \noindent Implement (from scratch!) linear regression using previous gradient descend code to optimize w1 and w0
        \begin{itemize}
            \item Input: a CSV file
            \item Output: w1, w0
            \item Print the intermediate iterative steps
        \end{itemize}
        \noindent Try experimenting with the previous example of house price
\end{abstract}

\chapter{How you implement the algorithm.}

\section{Mathematical Background}
Linear regression aims to model the relationship between a dependent variable and one or more independent variables by fitting a linear equation to the data. For a simple linear regression, we have:

\begin{equation}
y = w_1 \cdot x + w_0
\end{equation}

\noindent where $y$ is the predicted value, $x$ is the input feature, $w_1$ is the slope, and $w_0$ is the y-intercept.

\noindent To find the optimal values of $w_0$ and $w_1$, we define a loss function that measures the difference between the predicted values and the actual values:

\begin{equation}
L(w_0, w_1) = \frac{1}{2} \sum_{i=1}^{m} (w_1 \cdot x_i + w_0 - y_i)^2
\end{equation}

\noindent Gradient descent minimizes this loss function by iteratively updating the parameters in the direction of the negative gradient:

\begin{equation}
w_0 = w_0 - \alpha \frac{\partial L}{\partial w_0} = w_0 - \alpha \sum_{i=1}^{m} (w_1 \cdot x_i + w_0 - y_i)
\end{equation}

\begin{equation}
w_1 = w_1 - \alpha \frac{\partial L}{\partial w_1} = w_1 - \alpha \sum_{i=1}^{m} (w_1 \cdot x_i + w_0 - y_i) \cdot x_i
\end{equation}

\noindent where $\alpha$ is the learning rate that controls the step size during optimization.

\section{Implementation Details}
\noindent In this exercise, I implemented linear regression using gradient descent from scratch in Python. The key components of the implementation are:

\subsection{Loss Function}
\noindent The mean squared error (MSE) loss function:

\begin{verbatim}
def loss(x, y, w0=0, w1=1):
    m = len(x)
    cost = 0.0
    for i in range(m):
        f_wb_i = w1 * x[i] + w0
        cost = cost + (f_wb_i - y[i])**2
    total_cost = (1 / (2)) * cost
    return total_cost
\end{verbatim}

\subsection{Gradient Computation}
\noindent Computing the gradients for both $w_0$ and $w_1$:

\begin{verbatim}
def gradient(x, y, w0=0, w1=1):
    n = len(x)
    dL_dw0 = 0.0
    dL_dw1 = 0.0
    for i in range(n):
        err = (w0 + w1 * x[i]) - y[i]
        dL_dw0 += err
        dL_dw1 += err * x[i]
    return dL_dw0, dL_dw1
\end{verbatim}

\subsection{Gradient Descent Algorithm}
\noindent The main gradient descent optimization procedure:

\begin{verbatim}
def gradient_descent(x, y, w0=0, w1=1, lr=0.01, epochs=1000):
    w0_history = []
    w1_history = []
    loss_history = []

    x = np.array(x)
    y = np.array(y)
    for i in range(epochs):
        dL_dw0, dL_dw1 = gradient(x, y, w0, w1)
    
        w0 -= lr * dL_dw0
        w1 -= lr * dL_dw1
    
        w0_history.append(w0)
        w1_history.append(w1)
        current_loss = loss(x, y, w0, w1)
        loss_history.append(current_loss)
    
        if i % (epochs // 10) == 0 or i == epochs - 1:
             print(f"Epoch {i:5d}: Loss = {current_loss:.4e}, w0 = {w0:.4f}, w1 = {w1:.4f}")

    return w0, w1, loss_history
\end{verbatim}

\subsection{Visualization Functions}
\noindent To visualize the results, I implemented two key plotting functions:

\begin{verbatim}
def plot_loss(loss_history):
    plt.plot(loss_history)
    plt.title("Loss over epochs")
    plt.xlabel("Epochs")
    plt.ylabel("Loss")
    plt.show()
    
def plot_data(x, y, w0, w1):    
    plt.scatter(x, y, color='blue', label='Data points')
    x_range = np.linspace(min(x), max(x), 100)
    y_range = w0 + w1 * x_range
    plt.plot(x_range, y_range, color='red', label='Regression line')
    plt.title("Linear Regression")
    plt.xlabel("x")
    plt.ylabel("y")
    plt.legend()
    plt.show()
\end{verbatim}

\section{Dataset}
\noindent For this experiment, I used a sample dataset representing house prices:

\begin{verbatim}
x_train = [30, 32.4138, 34.8276, 37.2414, 39.6552]
y_train = [448.524, 509.248, 535.104, 551.432, 623.418]
\end{verbatim}

\noindent where $x$ represents the house size (in square meters) and $y$ represents the house price (in thousands of dollars).

\section{Experimental Setup}
\noindent To analyze the effect of different learning rates on convergence, I conducted experiments with various learning rates:

\begin{verbatim}
learning_rates = [0.1, 0.01, 0.001, 0.0001]
epochs = 50
initial_w0 = 0.0
initial_w1 = 1.0
\end{verbatim}

\section{Analysis of Results}
\noindent The experiments revealed significant differences in convergence behavior across different learning rates:

\subsection{Learning Rate = 0.001}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\linewidth]{image/lr-loss-func-0001.png}
    \caption{Loss Function with Learning Rate 0.001}
    \label{fig:enter-label}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\linewidth]{image/linear-res-0001.png}
    \caption{Linear Regression with Learning Rate 0.001}
    \label{fig:enter-label}
\end{figure}

\noindent Using a learning rate of 0.001 resulted in a diverging loss function. The parameters oscillated without converging to a minimum, indicating that the learning rate was still too large (predictably).

\subsection{Learning Rate = 0.0001} 
\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\linewidth]{image/lr-loss-00001.png}
    \caption{Loss Function with Learning Rate 0.0001}
    \label{fig:enter-label}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\linewidth]{image/lienar-res-00001.png}
    \caption{Linear Regression with Learning Rate 0.0001}
    \label{fig:enter-label}
\end{figure}

\noindent A learning rate of 0.0001 resulted in a very good convergence, which we have implemented in the Labwork 1. The convergence is fast (after 3 epochs) and then the loss function is stable. We should achieve the \textbf{w0} and \textbf{w1} values of 0.3995 and 15.3170 respectively. 

\section{Prediction Performance}
\noindent Using the optimized parameters ($w_0 = 0.3995$, $w_1 = 15.3170$) from the best model (learning rate = 0.01), we can make predictions for new data points. For example, for a house with 35 square meters:

\begin{verbatim}
predicted_price = 0.3995 + 15.3170 * 35 = 536.4945 (thousand dollars)
\end{verbatim}

\noindent And this is the result of the prediction using the model:
\begin{verbatim}
  Predicted value for x=35: 536.4940
\end{verbatim}
\noindent This indicates that the model predicts a house price of approximately \$536,494 for a 35 square meter house.

\section{Conclusion}
\noindent This implementation demonstrates the core concepts of linear regression implemented from scratch using gradient descent. The choice of learning rate is crucial for convergenceâ€”too large and the algorithm may diverge, too small and convergence becomes impractically slow. For this specific housing price dataset, a learning rate of 0.0001 provided the best balance between convergence speed and stability.

\clearpage
\pagenumbering{gobble}
\end{document}