\documentclass[hidelinks]{report}
\usepackage{graphicx} % Required for inserting images
\usepackage{amsmath}
\usepackage{siunitx}
\usepackage{placeins}
\usepackage{float}
\usepackage{hyperref}
\usepackage{cite}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{xcolor,graphicx}
\setcounter{secnumdepth}{0}
\usepackage{titlesec}
\usepackage[left=1.5in,top=1in,bottom=1in,right=1in]{geometry}
\usepackage{rotating}
\usepackage{subcaption}
\usepackage{lipsum}
\usepackage{fancyhdr}
\usepackage{mathptmx}
\usepackage{listings}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{adjustbox}
\usepackage{enumitem}
\usepackage{setspace}
\usepackage{multirow}
\usepackage{tikz}
\setcounter{tocdepth}{4}
\usepackage{amsfonts}
\usepackage{pdflscape}
\usepackage{booktabs} 
\usepackage{array} 

\newcommand{\mynote}[2]{\fbox{\bfseries\sffamily\scriptsize{#1}} {\small\textsf{\emph{#2}}}}

\newcommand{\hieplnc}[1]{\textcolor{red}{\mynote{hieplnc}{#1}}}

\newcommand{\sontg}[1]{\textcolor{blue}{\mynote{sontg}{#1}}}

\definecolor{blue}{RGB}{31,56,100}

\usepackage{lipsum}% http://ctan.org/pkg/lipsum
\makeatletter
\def\@makechapterhead#1{%
  {%
    \parindent \z@ \normalfont % No specific alignment for full justification
    
    \ifnum \c@secnumdepth >\m@ne
        \LARGE\bfseries \thechapter.\ % Changed to \Large (smaller than \LARGE)
    \fi
    \interlinepenalty\@M
    \LARGE\bfseries % Reduced from \LARGE to \Large
    #1\par\nobreak% <------------------ Chapter title
    \vskip 40\p@% <------------------ Space between chapter title and first paragraph
  }%
}

\def\@makeschapterhead#1{%
  {%
    \parindent \z@ \normalfont % No specific alignment for full justification
    \interlinepenalty\@M
    \LARGE\bfseries % Reduced from \LARGE to \Large
    #1\par\nobreak% <------------------ Chapter title
    \vskip 40\p@% <------------------ Space between chapter title and first paragraph
  }%
}
\makeatother

% Redefine the \thesection and \thesubsection representations
\renewcommand{\thesection}{\arabic{chapter}.\arabic{section}}
\renewcommand{\thesubsection}{\thesection.\arabic{subsection}}
\renewcommand{\thesubsubsection}{\thesubsection.\arabic{subsubsection}}

% Define a new counter for subsections
% \newcounter{subsecindex}[section]
% \renewcommand{\thesubsecindex}{\thesubsection%
%   \ifnum\value{subsecindex}>0
%     .\arabic{subsecindex}%
%   \fi
% }

% Redefine the \section command to include the index
% \let\oldsection\section
% \renewcommand{\section}[1]{%
%   \setcounter{subsecindex}{0} % Reset subsection counter for each section
%   \refstepcounter{section}%
%   \oldsection{\thesection\hspace{0.5em}#1}%
% }

% Redefine the \subsection command to include the index
% \let\oldsubsection\subsection
% \renewcommand{\subsection}[1]{%
%   \refstepcounter{subsection}%
%   \oldsubsection{\thesubsecindex\hspace{0.5em}#1}%
% }

% Redefine the \subsubsection command to include the index
% \let\oldsubsubsection\subsubsection
% \renewcommand{\subsubsection}[1]{%
%   \refstepcounter{subsubsection}%
%   \oldsubsubsection{\thesubsecindex\hspace{0.5em}#1}%
% }

\titleformat{\section}
  {\normalfont\LARGE\bfseries} % Adjust \Large to any size you prefer
  {\thesection}{3em}{}
\titleformat{\subsection}
  {\normalfont\Large\bfseries} % Adjust \Large to any size you prefer
  {\thesubsection}{3em}{}
\titleformat{\subsubsection}
  {\normalfont\Large\bfseries} % Adjust \Large to any size you prefer
  {\thesubsubsection}{3em}{}

\begin{document}
\pagenumbering{gobble}

\pdfbookmark[0]{Main Title}{maintitle}
\begin{titlepage}
    \begin{tikzpicture}[remember picture,overlay,inner sep=0,outer sep=0]
        \draw[black!70!black,line width=1.5pt]
            ([xshift=-0.65in,yshift=-1cm]current page.north east) coordinate (A) -- % Adjusted x-shift and y-shift
            ([xshift=0.65in,yshift=-1cm]current page.north west) coordinate (B) -- % Adjusted x-shift and y-shift
            ([xshift=0.65in,yshift=1cm]current page.south west) coordinate (C) -- % Adjusted x-shift and y-shift
            ([xshift=-0.65in,yshift=1cm]current page.south east) -- % Adjusted x-shift
            cycle;
    \end{tikzpicture}

    \begin{center}
        \huge \uppercase{University of Science and Technology of Hanoi} \\ [1.5 cm]
    
        \includegraphics[width=0.9\linewidth]{image/usth.png} \\[1cm]

        {\huge \bfseries \uppercase{MASTER LABWORK}} \\[1cm]

        {\large \bfseries Nguyen Viet Tung} \\ [0.5cm]
        {\large \bfseries 2440049} \\ [0.7cm]
        {\huge \bfseries \uppercase{Deep Learning}} \\[1cm]
        
        % Title
        \rule{\linewidth}{0.3mm} \\[0.4cm]
        { \Huge \bfseries\color{blue} Labwork 2: Linear Regression \\[0.4cm] }
        \rule{\linewidth}{0.3mm} \\[0.7cm]
        
        \large Academic Year: 2024-2026
    \end{center}

\end{titlepage}

\newpage
\pagenumbering{roman}
\tableofcontents

\newpage
\listoffigures

\thispagestyle{empty}
\newpage
\pagenumbering{arabic}

% Keep the preamble and title page sections as they are...

\begin{abstract}
        \noindent Implement (from scratch!) linear regression using previous gradient descend code to optimize w1 and w0
        \begin{itemize}
            \item Input: a CSV file
            \item Output: w1, w0
            \item Print the intermediate iterative steps
        \end{itemize}
        \noindent Try experimenting with the previous example of house price
\end{abstract}

\chapter{How you implement the algorithm.}

\section{Mathematical Background}
Linear regression aims to model the relationship between a dependent variable and one or more independent variables by fitting a linear equation to the data. For a simple linear regression, we have:

\begin{equation}
y = w_1 \cdot x + w_0
\end{equation}

\noindent where $y$ is the predicted value, $x$ is the input feature, $w_1$ is the slope, and $w_0$ is the y-intercept.

\noindent To find the optimal values of $w_0$ and $w_1$, we define a loss function that measures the difference between the predicted values and the actual values:

\begin{equation}
L(w_0, w_1) = \frac{1}{2} \sum_{i=1}^{m} (w_1 \cdot x_i + w_0 - y_i)^2
\end{equation}

\noindent Gradient descent minimizes this loss function by iteratively updating the parameters in the direction of the negative gradient:

\begin{equation}
w_0 = w_0 - \alpha \frac{\partial L}{\partial w_0} = w_0 - \alpha \sum_{i=1}^{m} (w_1 \cdot x_i + w_0 - y_i)
\end{equation}

\begin{equation}
w_1 = w_1 - \alpha \frac{\partial L}{\partial w_1} = w_1 - \alpha \sum_{i=1}^{m} (w_1 \cdot x_i + w_0 - y_i) \cdot x_i
\end{equation}

\noindent where $\alpha$ is the learning rate that controls the step size during optimization.

\section{Implementation Details}
\noindent In this exercise, I implemented linear regression using gradient descent from scratch in Python. The key components of the implementation are:

\subsection{Loss Function}
\noindent The mean squared error (MSE) loss function:

\begin{verbatim}
def loss(x, y, w0=0, w1=1):
    m = len(x)
    cost = 0.0
    for i in range(m):
        f_wb_i = w1 * x[i] + w0
        cost = cost + (f_wb_i - y[i])**2
    total_cost = (1 / (2)) * cost
    return total_cost
\end{verbatim}

\subsection{Gradient Computation}
\noindent Computing the gradients for both $w_0$ and $w_1$:

\begin{verbatim}
def gradient(x, y, w0=0, w1=1):
    n = len(x)
    dL_dw0 = 0.0
    dL_dw1 = 0.0
    for i in range(n):
        err = (w0 + w1 * x[i]) - y[i]
        dL_dw0 += err
        dL_dw1 += err * x[i]
    return dL_dw0, dL_dw1
\end{verbatim}

\subsection{Gradient Descent Algorithm}
\noindent The main gradient descent optimization procedure:

\begin{verbatim}
def gradient_descent(x, y, w0=0, w1=1, lr=0.01, epochs=1000):
    w0_history = []
    w1_history = []
    loss_history = []

    x = np.array(x)
    y = np.array(y)
    for i in range(epochs):
        dL_dw0, dL_dw1 = gradient(x, y, w0, w1)
    
        w0 -= lr * dL_dw0
        w1 -= lr * dL_dw1
    
        w0_history.append(w0)
        w1_history.append(w1)
        current_loss = loss(x, y, w0, w1)
        loss_history.append(current_loss)
    
        if i % (epochs // 10) == 0 or i == epochs - 1:
             print(f"Epoch {i:5d}: Loss = {current_loss:.4e}, w0 = {w0:.4f}, w1 = {w1:.4f}")

    return w0, w1, loss_history
\end{verbatim}

\subsection{Visualization Functions}
\noindent To visualize the results, I implemented two key plotting functions:

\begin{verbatim}
def plot_loss(loss_history):
    plt.plot(loss_history)
    plt.title("Loss over epochs")
    plt.xlabel("Epochs")
    plt.ylabel("Loss")
    plt.show()
    
def plot_data(x, y, w0, w1):    
    plt.scatter(x, y, color='blue', label='Data points')
    x_range = np.linspace(min(x), max(x), 100)
    y_range = w0 + w1 * x_range
    plt.plot(x_range, y_range, color='red', label='Regression line')
    plt.title("Linear Regression")
    plt.xlabel("x")
    plt.ylabel("y")
    plt.legend()
    plt.show()
\end{verbatim}

\section{Dataset}
\noindent For this experiment, I used a sample dataset representing house prices:

\begin{verbatim}
x_train = [30, 32.4138, 34.8276, 37.2414, 39.6552]
y_train = [448.524, 509.248, 535.104, 551.432, 623.418]
\end{verbatim}

\noindent where $x$ represents the house size (in square meters) and $y$ represents the house price (in thousands of dollars).

\section{Experimental Setup}
\noindent To analyze the effect of different learning rates on convergence, I conducted experiments with various learning rates:

\begin{verbatim}
learning_rates = [0.1, 0.01, 0.001, 0.0001]
epochs = 50
initial_w0 = 0.0
initial_w1 = 1.0
\end{verbatim}

\section{Analysis of Results}
\noindent The experiments revealed significant differences in convergence behavior across different learning rates:

\subsection{Learning Rate = 0.001}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\linewidth]{image/lr-loss-func-0001.png}
    \caption{Loss Function with Learning Rate 0.001}
    \label{fig:enter-label}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\linewidth]{image/linear-res-0001.png}
    \caption{Linear Regression with Learning Rate 0.001}
    \label{fig:enter-label}
\end{figure}

\noindent Using a learning rate of 0.001 resulted in a diverging loss function. The parameters oscillated without converging to a minimum, indicating that the learning rate was still too large (predictably).

\subsection{Learning Rate = 0.0001} 
\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\linewidth]{image/lr-loss-00001.png}
    \caption{Loss Function with Learning Rate 0.0001}
    \label{fig:enter-label}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\linewidth]{image/lienar-res-00001.png}
    \caption{Linear Regression with Learning Rate 0.0001}
    \label{fig:enter-label}
\end{figure}

\noindent A learning rate of 0.0001 resulted in a very good convergence, which we have implemented in the Labwork 1. The convergence is fast (after 3 epochs) and then the loss function is stable. We should achieve the \textbf{w0} and \textbf{w1} values of 0.3995 and 15.3170 respectively. 

\section{Prediction Performance}
\noindent Using the optimized parameters ($w_0 = 0.3995$, $w_1 = 15.3170$) from the best model (learning rate = 0.01), we can make predictions for new data points. For example, for a house with 35 square meters:

\begin{verbatim}
predicted_price = 0.3995 + 15.3170 * 35 = 536.4945 (thousand dollars)
\end{verbatim}

\noindent And this is the result of the prediction using the model:
\begin{verbatim}
  Predicted value for x=35: 536.4940
\end{verbatim}
\noindent This indicates that the model predicts a house price of approximately \$536,494 for a 35 square meter house.

\section{Conclusion}
\noindent This implementation demonstrates the core concepts of linear regression implemented from scratch using gradient descent. The choice of learning rate is crucial for convergence—too large and the algorithm may diverge, too small and convergence becomes impractically slow. For this specific housing price dataset, a learning rate of 0.0001 provided the best balance between convergence speed and stability.

\clearpage
\pagenumbering{gobble}
\end{document}